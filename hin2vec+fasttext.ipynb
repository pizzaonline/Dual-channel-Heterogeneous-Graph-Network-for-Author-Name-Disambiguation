{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "import json\n",
    "from os.path import join\n",
    "import pickle\n",
    "import re\n",
    "from gensim.models import FastText\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "file = open('/home/cyj/公开数据/all_text.txt',\"r\") \n",
    "list001=[]\n",
    "for line in file:\n",
    "    line = line.strip('\\n')\n",
    "    list001.append(line) # do somethingfile.close()\n",
    "file.close()\n",
    "list002=[]\n",
    "for i in range(len(list001)):\n",
    "    l=list001[i].split(' ')\n",
    "    for j in range(len(l)):\n",
    "        list002.append(l[j])\n",
    "from collections import Counter\n",
    "b = Counter(list002)\n",
    "u=list(b.keys())\n",
    "t=list(b.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ty1=len(list002)\n",
    "save_model_name = \"/home/cyj/公开数据/fasttextmodel/FASTTEXT031401.model\"\n",
    "model_w = FastText.load(save_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################# Load and Save Data ################\n",
    "\n",
    "def load_json(rfdir, rfname):\n",
    "    with codecs.open(join(rfdir, rfname), 'r', encoding='utf-8') as rf:\n",
    "        return json.load(rf)\n",
    "\n",
    "\n",
    "def dump_json(obj, wfpath, wfname, indent=None):\n",
    "    with codecs.open(join(wfpath, wfname), 'w', encoding='utf-8') as wf:\n",
    "        json.dump(obj, wf, ensure_ascii=False, indent=indent)\n",
    "\n",
    "\n",
    "\n",
    "def dump_data(obj, wfpath, wfname):\n",
    "    with open(os.path.join(wfpath, wfname), 'wb') as wf:\n",
    "        pickle.dump(obj, wf)\n",
    "\n",
    "\n",
    "def load_data(rfpath, rfname):\n",
    "    with open(os.path.join(rfpath, rfname), 'rb') as rf:\n",
    "        return pickle.load(rf)\n",
    "    \n",
    "################# Compare Lists ################\n",
    "\n",
    "def tanimoto(p,q):\n",
    "    c = [v for v in p if v in q]\n",
    "    return float(len(c) / (len(p) + len(q) - len(c)))\n",
    "\n",
    "\n",
    "\n",
    "################# Paper similarity ################\n",
    "\n",
    "def generate_pair(pubs,outlier): ##求匹配相似度\n",
    "    dirpath = '/home/cyj/hin+A+O/f1'\n",
    "    \n",
    "    paper_org = {}\n",
    "    paper_conf = {}\n",
    "    paper_author = {}\n",
    "    paper_word = {}\n",
    "    \n",
    "    temp=set()\n",
    "    with open(dirpath + \"/paper_org.txt\", encoding='utf-8') as pafile:\n",
    "        for line in pafile:\n",
    "            temp.add(line)                       \n",
    "    for line in temp: \n",
    "        toks = line.strip().split(\"\\t\")\n",
    "        if len(toks) == 2:\n",
    "            p, a = toks[0], toks[1]\n",
    "            if p not in paper_org:\n",
    "                paper_org[p] = []\n",
    "            paper_org[p].append(a)\n",
    "    temp.clear()\n",
    "    \n",
    "    with open(dirpath + \"/paper_conf.txt\", encoding='utf-8') as pafile:\n",
    "        for line in pafile:\n",
    "            temp.add(line)                       \n",
    "    for line in temp: \n",
    "        toks = line.strip().split(\"\\t\")\n",
    "        if len(toks) == 2:\n",
    "            p, a = toks[0], toks[1]\n",
    "            if p not in paper_conf:\n",
    "                paper_conf[p]=[]\n",
    "            paper_conf[p]=a\n",
    "    temp.clear()\n",
    "    \n",
    "    with open(dirpath + \"/paper_author.txt\", encoding='utf-8') as pafile:\n",
    "        for line in pafile:\n",
    "            temp.add(line)                       \n",
    "    for line in temp: \n",
    "        toks = line.strip().split(\"\\t\")\n",
    "        if len(toks) == 2:\n",
    "            p, a = toks[0], toks[1]\n",
    "            if p not in paper_author:\n",
    "                paper_author[p] = []\n",
    "            paper_author[p].append(a)\n",
    "    temp.clear()\n",
    "       \n",
    "    with open(dirpath + \"/paper_word.txt\", encoding='utf-8') as pafile:\n",
    "        for line in pafile:\n",
    "            temp.add(line)                       \n",
    "    for line in temp: \n",
    "        toks = line.strip().split(\"\\t\")\n",
    "        if len(toks) == 2:\n",
    "            p, a = toks[0], toks[1]\n",
    "            if p not in paper_word:\n",
    "                paper_word[p] = []\n",
    "            paper_word[p].append(a)\n",
    "    temp.clear()\n",
    "    \n",
    "    \n",
    "    paper_paper = np.zeros((len(pubs),len(pubs)))\n",
    "    for i,pid in enumerate(pubs):\n",
    "        if i not in outlier:\n",
    "            continue\n",
    "        for j,pjd in enumerate(pubs):\n",
    "            if j==i:\n",
    "                continue\n",
    "            ca=0\n",
    "            cv=0\n",
    "            co=0\n",
    "            ct=0\n",
    "          \n",
    "            if pid in paper_author and pjd in paper_author:\n",
    "                ca = len(set(paper_author[pid])&set(paper_author[pjd]))*1.5\n",
    "            if pid in paper_conf and pjd in paper_conf and 'null' not in paper_conf[pid]:\n",
    "                cv = tanimoto(set(paper_conf[pid]),set(paper_conf[pjd]))\n",
    "            if pid in paper_org and pjd in paper_org:\n",
    "                co = tanimoto(set(paper_org[pid]),set(paper_org[pjd]))\n",
    "            if pid in paper_word and pjd in paper_word:\n",
    "                ct = len(set(paper_word[pid])&set(paper_word[pjd]))/3\n",
    "                    \n",
    "            paper_paper[i][j] =ca+cv+co+ct\n",
    "            \n",
    "    return paper_paper\n",
    "\n",
    "    \n",
    "        \n",
    "################# Evaluate ################\n",
    "        \n",
    "def pairwise_evaluate(correct_labels,pred_labels):\n",
    "    TP = 0.0  # Pairs Correctly Predicted To SameAuthor\n",
    "    TP_FP = 0.0  # Total Pairs Predicted To SameAuthor\n",
    "    TP_FN = 0.0  # Total Pairs To SameAuthor\n",
    "    list1=[]\n",
    "    list2=[]\n",
    "    list3=[]\n",
    "    for i in range(len(correct_labels)):\n",
    "        for j in range(i + 1, len(correct_labels)):\n",
    "            if correct_labels[i] == correct_labels[j]:\n",
    "                TP_FN += 1\n",
    "            if pred_labels[i] == pred_labels[j]:\n",
    "                TP_FP += 1\n",
    "            if (correct_labels[i] == correct_labels[j]) and (pred_labels[i] == pred_labels[j]):\n",
    "                TP += 1\n",
    "    list1.append(TP)\n",
    "    list2.append(TP_FP)\n",
    "    list3.append(TP_FN)\n",
    "\n",
    "    if TP == 0:\n",
    "        pairwise_precision = 0\n",
    "        pairwise_recall = 0\n",
    "        pairwise_f1 = 0\n",
    "    else:\n",
    "        pairwise_precision = TP / TP_FP\n",
    "        pairwise_recall = TP / TP_FN\n",
    "        pairwise_f1 = (2 * pairwise_precision * pairwise_recall) / (pairwise_precision + pairwise_recall)\n",
    "    return pairwise_precision, pairwise_recall, pairwise_f1, list1, list2, list3\n",
    "\n",
    "\n",
    "################# Save Paper Features ################\n",
    "\n",
    "def save_relation(name_pubs_raw, name): # 保存论文的各种feature\n",
    "    name_pubs_raw = load_json('/home/cyj/hin+A+O/result', name_pubs_raw)\n",
    "    ## trained by all text in the datasets. Training code is in the cells of \"train word2vec\"\n",
    "    #save_model_name = \"/home/cyj/公开数据/word2vecmodel/Aword2vec.model\"\n",
    "    #model_w = word2vec.Word2Vec.load(save_model_name)\n",
    "    save_model_name = \"/home/cyj/公开数据/fasttextmodel/FASTTEXT031401.model\"\n",
    "    model_w = FastText.load(save_model_name)\n",
    "    \n",
    "    r = '[!“”\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~—～’]+'\n",
    "    stopword = ['at','based','in','of','for','on','and','to','an','using','with','the','by','we','be','is','are','can']\n",
    "    stopword1 = ['university','univ','china','department','dept','laboratory','lab','school','al','et',\n",
    "                 'institute','inst','college','chinese','beijing','journal','science','international']\n",
    "    \n",
    "    f1 = open ('/home/cyj/hin+A+O/f1/paper_author.txt','w',encoding = 'utf-8')\n",
    "    f2 = open ('/home/cyj/hin+A+O/f1/paper_conf.txt','w',encoding = 'utf-8')\n",
    "    f3 = open ('/home/cyj/hin+A+O/f1/paper_word.txt','w',encoding = 'utf-8')\n",
    "    f4 = open ('/home/cyj/hin+A+O/f1/paper_org.txt','w',encoding = 'utf-8')\n",
    "    f5 = open ('/home/cyj/公开数据/database/英文停用词表.txt','r')\n",
    "    strchword = f5.read()     #将txt文件的所有内容读入到字符串str1中\n",
    "    f5.close()\n",
    "    \n",
    "    taken = name.split(\"_\")\n",
    "    name = taken[0] + taken[1]\n",
    "    name_reverse = taken[1]  + taken[0]\n",
    "    if len(taken)>2:\n",
    "        name = taken[0] + taken[1] + taken[2]\n",
    "        name_reverse = taken[2]  + taken[0] + taken[1]\n",
    "    \n",
    "    authorname_dict={}\n",
    "    ptext_emb = {}  \n",
    "    \n",
    "    tcp=set()  \n",
    "    for i,pid in enumerate(name_pubs_raw):\n",
    "        \n",
    "        pub = name_pubs_raw[pid]\n",
    "        \n",
    "        #save authors\n",
    "        org=\"\"\n",
    "        for author in pub[\"authors\"]:\n",
    "            authorname = re.sub(r,'', author[\"name\"]).lower()\n",
    "            taken = authorname.split(\" \")\n",
    "            if len(taken)==2: ##检测目前作者名是否在作者词典中\n",
    "                authorname = taken[0] + taken[1]\n",
    "                authorname_reverse = taken[1]  + taken[0] \n",
    "            \n",
    "                if authorname not in authorname_dict:\n",
    "                    if authorname_reverse not in authorname_dict:\n",
    "                        authorname_dict[authorname]=1\n",
    "                    else:\n",
    "                        authorname = authorname_reverse \n",
    "            else:\n",
    "                authorname = authorname.replace(\" \",\"\")\n",
    "            \n",
    "            if authorname!=name and authorname!=name_reverse:\n",
    "                f1.write(pid + '\\t' + authorname + '\\n')\n",
    "        \n",
    "            else:\n",
    "                if \"org\" in author:\n",
    "                    org = author[\"org\"]\n",
    "                    \n",
    "                    \n",
    "        #save org 待消歧作者的机构名\n",
    "        pstr = org.strip()\n",
    "        pstr = pstr.lower() #小写\n",
    "        pstr = re.sub(r,' ', pstr) #去除符号\n",
    "        pstr = re.sub(r'\\s{2,}', ' ', pstr).strip() #去除多余空格\n",
    "        pstr = pstr.split(' ')\n",
    "        pstr = [word for word in pstr if len(word)>1]\n",
    "        pstr = [word for word in pstr if word not in stopword1]\n",
    "        pstr = [word for word in pstr if word not in stopword]\n",
    "        pstr = [word for word in pstr if word not in strchword]\n",
    "        pstr=set(pstr)\n",
    "        for word in pstr:\n",
    "            f4.write(pid + '\\t' + word + '\\n')\n",
    "\n",
    "        \n",
    "        #save venue\n",
    "        pstr = pub[\"venue\"].strip()\n",
    "        pstr = pstr.lower()\n",
    "        pstr = re.sub(r,' ', pstr)\n",
    "        pstr = re.sub(r'\\s{2,}', ' ', pstr).strip()\n",
    "        pstr = pstr.split(' ')\n",
    "        pstr = [word for word in pstr if len(word)>1]\n",
    "        pstr = [word for word in pstr if word not in stopword1]\n",
    "        pstr = [word for word in pstr if word not in stopword]\n",
    "        pstr = [word for word in pstr if word not in strchword]\n",
    "        for word in pstr:\n",
    "            f2.write(pid + '\\t' + word + '\\n')\n",
    "        if len(pstr)==0:\n",
    "            f2.write(pid + '\\t' + 'null' + '\\n')\n",
    "\n",
    "            \n",
    "        #save text\n",
    "        pstr = \"\"    \n",
    "        keyword=\"\"\n",
    "        if \"keywords\" in pub:\n",
    "            for word in pub[\"keywords\"]:\n",
    "                keyword=keyword+word+\" \"\n",
    "        pstr = pstr + pub[\"title\"]\n",
    "        pstr=pstr.strip()\n",
    "        pstr = pstr.lower()\n",
    "        pstr = re.sub(r,' ', pstr)\n",
    "        pstr = re.sub(r'\\s{2,}', ' ', pstr).strip()\n",
    "        pstr = pstr.split(' ')\n",
    "        pstr = [word for word in pstr if len(word)>1]\n",
    "        pstr = [word for word in pstr if word not in stopword1]\n",
    "        pstr = [word for word in pstr if word not in stopword]\n",
    "        pstr = [word for word in pstr if word not in strchword]\n",
    "        for word in pstr:\n",
    "            f3.write(pid + '\\t' + word + '\\n')\n",
    "        \n",
    "        #save all words' embedding\n",
    "        pstr = keyword + \" \" + pub[\"title\"] + \" \" + pub[\"venue\"] + \" \" + org\n",
    "        if \"year\" in pub:\n",
    "              pstr = pstr +  \" \" + str(pub[\"year\"])\n",
    "        pstr=pstr.strip()\n",
    "        pstr = pstr.lower()\n",
    "        pstr = re.sub(r,' ', pstr)\n",
    "        pstr = re.sub(r'\\s{2,}', ' ', pstr).strip()\n",
    "        pstr = pstr.split(' ')\n",
    "        pstr = [word for word in pstr if len(word)>2]\n",
    "        pstr = [word for word in pstr if word not in stopword]\n",
    "        pstr = [word for word in pstr if word not in stopword1]\n",
    "        pstr = [word for word in pstr if word not in strchword]\n",
    "\n",
    "        words_vec=[]\n",
    "        for word in pstr:\n",
    "            if ((word in model_w) and (word in u)):\n",
    "                ty2=u.index(word)\n",
    "                ty3=t[ty2]\n",
    "                m1=1-(ty3/ty1)\n",
    "                words_vec.append(model_w[word]*m1)\n",
    "        if len(words_vec)<1:\n",
    "            words_vec.append(np.zeros(100))\n",
    "            tcp.add(i)\n",
    "            #print ('outlier:',pid,pstr)\n",
    "        ptext_emb[pid] = np.mean(words_vec,0)\n",
    "        \n",
    "    #  ptext_emb: key is paper id, and the value is the paper's text embedding\n",
    "    dump_data(ptext_emb,'/home/cyj/hin+A+O/f1','ptext_emb.pkl')\n",
    "    # the paper index that lack text information\n",
    "    dump_data(tcp,'/home/cyj/hin+A+O/f1','tcp.pkl')\n",
    "            \n",
    " \n",
    "    f1.close()\n",
    "    f2.close()\n",
    "    f3.close()\n",
    "    f4.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def save_generatenode(name_pubs_raw, name): # 保存论文的各种feature\n",
    "    pubs_raw = load_json('/home/cyj/hin+A+O/result', name_pubs_raw)\n",
    "    #pubs_raw = load_json(\"/home/cyj/数据/database\",\"li_guo.json\")\n",
    "    pubs_raw1=list(pubs_raw.keys())\n",
    "    pubs_raw2=list(pubs_raw.values())\n",
    "    list01=[]\n",
    "    list02=[]\n",
    "    list03=[]\n",
    "    list04=[]\n",
    "    for i in range(len(pubs_raw2)):\n",
    "        a = pubs_raw2[i]['authors']\n",
    "        for j in range(len(a)):\n",
    "            list01.append(a[j]['name'])\n",
    "            t=list(a[j].keys())\n",
    "            if len(t) == 1:\n",
    "                list02.append('')\n",
    "            #elif t[1] == 'org':\n",
    "            else:\n",
    "                list02.append(a[j]['org'])\n",
    "        list03.append(list01)\n",
    "        list04.append(list02)\n",
    "        list01=[]\n",
    "        list02=[]\n",
    "    org1=[]\n",
    "    org2=[]\n",
    "    for i in range(len(list04)):\n",
    "        for j in range(len(list04[i])):\n",
    "            #if list04[i][j] != '':\n",
    "                org1.append(list04[i][j])\n",
    "        org2.append(org1)\n",
    "        org1=[]\n",
    "    org3=[]\n",
    "    for i in range(len(org2)):\n",
    "        if org2[i] != []:\n",
    "            org3.append(org2[i])\n",
    "    aut=[]\n",
    "    aut1=[]\n",
    "    for i in range(len(list03)):\n",
    "        for j in range(len(list03[i])):\n",
    "            rule = re.compile(r'[^a-zA-Z0-9\\u4e00-\\u9fa5]')\n",
    "            line = rule.sub('',list03[i][j]).lower()\n",
    "            aut1.append(line)\n",
    "        aut.append(aut1)\n",
    "        aut1=[]\n",
    "    org4 = []\n",
    "    org5 = []\n",
    "    for i in range(len(aut)):\n",
    "        for j in range(len(aut[i])):\n",
    "            name1=name.replace('_','').lower()\n",
    "            if aut[i][j] != name1:\n",
    "                org4.append(org3[i][j])\n",
    "        org5.append(org4)\n",
    "        org4 = []\n",
    "    org6=[]\n",
    "    org7=[]\n",
    "    for i in range(len(org5)):\n",
    "        for j in range(len(org5[i])):\n",
    "            if org5[i][j] != '':\n",
    "                org6.append(org5[i][j])\n",
    "        org7.append(org6)\n",
    "        org6=[]\n",
    "    org9=[]\n",
    "    org10=[]\n",
    "    for i in range(len(org7)):\n",
    "        if org7[i] != []:\n",
    "            org9.append(org7[i])\n",
    "            org10.append(pubs_raw1[i])\n",
    "    r = '[!“”\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~—～’]+'\n",
    "    stopword = ['at','based','in','of','for','on','and','to','an','using','with','the','by','we','be','is','are','can']\n",
    "    stopword1 = ['university','univ','china','department','dept','laboratory','lab','school','al','et',\n",
    "                 'institute','inst','college','chinese','beijing','journal','science','international']\n",
    "    f5 = open ('/home/cyj/公开数据/database/英文停用词表.txt','r')\n",
    "    strchword = f5.read()     #将txt文件的所有内容读入到字符串str1中\n",
    "    f5.close()\n",
    "    org11=[]\n",
    "    for i in range(len(org9)):\n",
    "        a=''.join(org9[i])\n",
    "        pstr = a.strip()\n",
    "        pstr = pstr.lower() #小写\n",
    "        pstr = re.sub(r,' ', pstr) #去除符号\n",
    "        pstr = re.sub(r'\\s{2,}', ' ', pstr).strip() #去除多余空格\n",
    "        pstr = pstr.split(' ')\n",
    "        pstr = [word for word in pstr if len(word)>1]\n",
    "        pstr = [word for word in pstr if word not in stopword1]\n",
    "        pstr = [word for word in pstr if word not in stopword]\n",
    "        pstr = [word for word in pstr if word not in strchword]\n",
    "        org11.append(pstr)\n",
    "    org12=[]\n",
    "    org13=[]\n",
    "    org14=[]\n",
    "    org15=[]\n",
    "    org16=[]\n",
    "    org17=[]\n",
    "    for i in range(len(org11)):\n",
    "        #print(i)\n",
    "        l=len(org11)\n",
    "        u=org11[i+1:l]\n",
    "        g=org10[i+1:l]\n",
    "        for j in range(len(u)):\n",
    "            x = [k for k in org11[i] if k in u[j]]\n",
    "            if len(x) != 0:\n",
    "                org12.append(pubs_raw1[i])\n",
    "                org13.append(g[j])\n",
    "                org14.append(len(x))\n",
    "                org15.append('Ti')\n",
    "                org16.append('Ti')\n",
    "                org17.append('Oi-Oi')\n",
    "    t1=[]\n",
    "    t2=[]\n",
    "    L=[]\n",
    "    t3=[]\n",
    "    t4=[]\n",
    "    t5=[]\n",
    "    for i in range(len(aut)):\n",
    "        l=len(aut)\n",
    "        u=aut[i+1:l]\n",
    "        g=pubs_raw1[i+1:l]\n",
    "        for j in range(len(u)):\n",
    "            x = [k for k in aut[i] if k in u[j]]\n",
    "            if len(x) != 1:\n",
    "                t1.append(pubs_raw1[i])\n",
    "                t2.append(g[j])\n",
    "                L.append(len(x)-1)\n",
    "                t3.append('Ti')\n",
    "                t4.append('Ti')\n",
    "                t5.append('Ai-Ai')\n",
    "    n1=org12+t1\n",
    "    n2=org13+t2\n",
    "    n3=org14+L\n",
    "    n4=org15+t3\n",
    "    n5=org16+t4\n",
    "    n6=org17+t5\n",
    "    array1 = np.array(n1)\n",
    "    df09=pd.DataFrame(array1)\n",
    "    df09.columns=[\"dest_node\"]\n",
    "    array2 = np.array(n2)\n",
    "    df010=pd.DataFrame(array2)\n",
    "    df010.columns=[\"source_node\"]\n",
    "    array3 = np.array(n3)\n",
    "    df011=pd.DataFrame(array3)\n",
    "    df011.columns=[\"weight\"]\n",
    "    array4 = np.array(n4)\n",
    "    df012=pd.DataFrame(array4)\n",
    "    df012.columns=[\"source_class\"]\n",
    "    array5 = np.array(n5)\n",
    "    df013=pd.DataFrame(array5)\n",
    "    df013.columns=[\"dest_class\"]\n",
    "    array6 = np.array(n6)\n",
    "    df014=pd.DataFrame(array6)\n",
    "    df014.columns=[\"edge_class\"]\n",
    "    df016=pd.concat([df09,df010,df011,df012,df013,df014],axis=1)\n",
    "    ll=t1+t2+org12+org13\n",
    "    uu=list(set(ll))\n",
    "    list007=[]\n",
    "    for i in range(len(pubs_raw1)):\n",
    "        if pubs_raw1[i] not in uu:\n",
    "            list007.append(pubs_raw1[i])\n",
    "    return df016,list007"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "from itertools import product\n",
    "from collections import defaultdict\n",
    "\n",
    "class HIN:\n",
    "    \"\"\"\n",
    "    Class to generate vertex sequences.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, window=None):\n",
    "        self.graph = nx.DiGraph()\n",
    "        self.node_size = 0\n",
    "        self._path_size = 0\n",
    "\n",
    "        def new_id():\n",
    "            i = self.node_size\n",
    "            self.node_size += 1\n",
    "            return i\n",
    "\n",
    "        self._node2id = defaultdict(new_id)\n",
    "        self._id2type = {}\n",
    "        self._window = window\n",
    "        self._node_types = set()\n",
    "        self._path2id = None\n",
    "        self._id2path = None\n",
    "        self._id2node = None\n",
    "\n",
    "    @property\n",
    "    def id2node(self):\n",
    "        return self._id2node\n",
    "\n",
    "    @property\n",
    "    def id2path(self):\n",
    "        return self._id2path\n",
    "\n",
    "    @property\n",
    "    def window(self):\n",
    "        return self._window\n",
    "\n",
    "    @window.setter\n",
    "    def window(self, val):\n",
    "        if not self._window:\n",
    "            self._window = val\n",
    "        else:\n",
    "            raise ValueError(\"window只能被设定一次\")\n",
    "\n",
    "    @property\n",
    "    def path_size(self):\n",
    "        if not self._path_size:\n",
    "            raise ValueError(\"run sample() first to count path size\")\n",
    "        return self._path_size\n",
    "\n",
    "    def add_edge(self, source_node, source_class, dest_node, dest_class, edge_class, weight):\n",
    "        i = self._node2id[source_node]\n",
    "        j = self._node2id[dest_node]\n",
    "        self._id2type[i] = source_class\n",
    "        self._id2type[j] = dest_class\n",
    "        self._node_types.add(source_class)\n",
    "        self._node_types.add(dest_class)\n",
    "        self.graph.add_edge(i, j, weight=weight)\n",
    "\n",
    "    def small_walk(self, start_node, length):\n",
    "        walk = [start_node]\n",
    "        for i in range(1, length):\n",
    "            if next(nx.neighbors(self.graph, walk[-1]), None) is None:\n",
    "                break\n",
    "            cur_node = walk[-1]\n",
    "            nodes = list(nx.neighbors(self.graph, cur_node))\n",
    "            weights = [self.graph[cur_node][i]['weight'] for i in nodes]  # 有向图可能不能这么做\n",
    "            s = sum(weights)\n",
    "            if s != 0:\n",
    "                weights = [i/s for i in weights]\n",
    "                #walk += random.choices(nodes, weights, k=1)\n",
    "                walk += random.sample(list(nx.neighbors(self.graph, cur_node)), 1)  # todo 添加按权重游走的采样方式\n",
    "        return walk\n",
    "\n",
    "    def do_walks(self, length):\n",
    "        for start_node in range(self.node_size):\n",
    "            yield self.small_walk(start_node, length)\n",
    "\n",
    "    def sample(self, length, n_repeat):\n",
    "        \"\"\"\n",
    "        从随机游走的结果中截取结果返回，每个节点轮流作为起始节点\n",
    "        :param length: 游走长度\n",
    "        :param n_repeat: 游走次数\n",
    "        :return: （start_id,end_id,edge_type)\n",
    "        \"\"\"\n",
    "        if not self.window:\n",
    "            raise ValueError(\"window not set\")\n",
    "\n",
    "        if not self._path2id:\n",
    "            self._path2id = {}\n",
    "            path_id = 0\n",
    "            for w in range(1, self._window + 1):\n",
    "                for i in product(self._node_types, repeat=w + 1):\n",
    "                    self._path2id[i] = path_id\n",
    "                    path_id += 1\n",
    "\n",
    "            self._path_size = len(self._path2id)\n",
    "            self._id2node = {v: k for k, v in self._node2id.items()}\n",
    "            self._id2path = {v: k for k, v in self._path2id.items()}\n",
    "\n",
    "        samples = []\n",
    "\n",
    "        for repeat in range(n_repeat):\n",
    "            for walk in self.do_walks(length):\n",
    "                cur_len = 0\n",
    "                for i, node_id in enumerate(walk):\n",
    "                    cur_len = min(cur_len + 1, self._window + 1)  # 当window=n的时候，最长路径有n+1个节点\n",
    "                    if cur_len >= 2:\n",
    "                        for path_length in range(1, cur_len):\n",
    "                            sample = (walk[i - path_length], walk[i],\n",
    "                                      self._path2id[tuple([self._id2type[t] for t in walk[i - path_length:i + 1]])])\n",
    "                            # print(tuple([self.id2type[t] for t in walk[i-path_length:i + 1]]))\n",
    "                            samples.append(sample)\n",
    "\n",
    "        return samples\n",
    "\n",
    "    def print_statistics(self):\n",
    "        print(f'size = {self.node_size}')\n",
    "        \n",
    "######################################构建图#######################3    \n",
    "def load_a_HIN_from_pandas(edges, print_graph=False):\n",
    "    \"\"\"\n",
    "    单向边：edges = list(pd.df)\n",
    "    \"\"\"\n",
    "\n",
    "    def reverse(df):\n",
    "        \"\"\"\n",
    "        reverse source & dest\n",
    "        \"\"\"\n",
    "        df = df.rename({'source_node': 'dest_node', 'dest_node': 'source_node',\n",
    "                        'source_class': 'dest_class', 'dest_class': 'source_class'},\n",
    "                       axis=1)\n",
    "        # reverse edge_class\n",
    "        df.edge_class = df.edge_class.map(lambda x: '-'.join(reversed(x.split('-'))))\n",
    "        return df\n",
    "\n",
    "    print('load graph from edges...')\n",
    "    g = HIN()\n",
    "    if isinstance(edges, list):\n",
    "        edges = pd.concat(edges, sort=False)\n",
    "    edges = edges.append(reverse(edges), sort=False, ignore_index=True)\n",
    "\n",
    "    for index, row in edges.iterrows():\n",
    "        g.add_edge(row['source_node'], row['source_class'],\n",
    "                   row['dest_node'], row['dest_class'], row['edge_class'],\n",
    "                   row['weight'])\n",
    "    if print_graph:\n",
    "        g.print_statistics()\n",
    "    print('finish loading graph!')\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class NSTrainSet(Dataset):\n",
    "    \"\"\"\n",
    "    完全随机的负采样 todo 改进一下？\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, sample, node_size, neg=5):\n",
    "        \"\"\"\n",
    "        :param node_size: 节点数目\n",
    "        :param neg: 负采样数目\n",
    "        :param sample: HIN.sample()返回值，(start_node, end_node, path_id)\n",
    "        \"\"\"\n",
    "\n",
    "        print('init training dataset...')\n",
    "\n",
    "        l = len(sample)\n",
    "\n",
    "        x = np.tile(sample, (neg + 1, 1))\n",
    "        y = np.zeros(l * (1 + neg))\n",
    "        y[:l] = 1\n",
    "\n",
    "        # x[l:, 2] = np.random.randint(0, path_size - 1, (l * neg,))\n",
    "        x[l:, 1] = np.random.randint(0, node_size - 1, (l * neg,))\n",
    "\n",
    "        self.x = torch.LongTensor(x)\n",
    "        self.y = torch.FloatTensor(y)\n",
    "        self.length = len(x)\n",
    "\n",
    "        print('finished')\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.x[index], self.y[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HIN2vec(nn.Module):\n",
    "\n",
    "    def __init__(self, node_size, path_size, embed_dim, sigmoid_reg=False, r=True):\n",
    "        super().__init__()\n",
    "\n",
    "        # self.args = args\n",
    "\n",
    "        def binary_reg(x: torch.Tensor):\n",
    "            raise NotImplementedError()\n",
    "            # return (x >= 0).float()  # do not have gradients\n",
    "\n",
    "        self.reg = torch.sigmoid if sigmoid_reg else binary_reg\n",
    "\n",
    "        self.__initialize_model(node_size, path_size, embed_dim, r)\n",
    "\n",
    "    def __initialize_model(self, node_size, path_size, embed_dim, r):\n",
    "        self.start_embeds = nn.Embedding(node_size, embed_dim)\n",
    "        self.end_embeds = self.start_embeds if r else nn.Embedding(node_size, embed_dim)\n",
    "\n",
    "        self.path_embeds = nn.Embedding(path_size, embed_dim)\n",
    "        # self.classifier = nn.Sequential(\n",
    "        #     nn.Linear(embed_dim, 1),\n",
    "        #     nn.Sigmoid(),\n",
    "        # )\n",
    "\n",
    "    def forward(self, start_node: torch.LongTensor, end_node: torch.LongTensor, path: torch.LongTensor):\n",
    "        # assert start_node.dim() == 1  # shape = (batch_size,)\n",
    "\n",
    "        s = self.start_embeds(start_node)  # (batch_size, embed_size)\n",
    "        e = self.end_embeds(end_node)\n",
    "        p = self.path_embeds(path)\n",
    "        p = self.reg(p)\n",
    "\n",
    "        agg = torch.mul(s, e)\n",
    "        agg = torch.mul(agg, p)\n",
    "        # agg = F.sigmoid(agg)\n",
    "        # output = self.classifier(agg)\n",
    "\n",
    "        output = torch.sigmoid(torch.sum(agg, axis=1))\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(log_interval, model, device, train_loader: DataLoader, optimizer, loss_function, epoch):\n",
    "    model = model.to(device)\n",
    "    model.train()\n",
    "    for idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data[:, 0], data[:, 1], data[:, 2])\n",
    "        loss = loss_function(output.view(-1), target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if idx % log_interval == 0:\n",
    "            print(f'\\rTrain Epoch: {epoch} '\n",
    "                  f'[{idx * len(data)}/{len(train_loader.dataset)} ({100. * idx / len(train_loader):.3f}%)]\\t'\n",
    "                  f'Loss: {loss.item():.3f}\\t\\t',\n",
    "                  # f'data = {data}\\t target = {target}',\n",
    "                  end='')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pubs_raw = load_json(\"/home/cyj/公开数据/database\",\"train_pub.json\")\n",
    "name_pubs = load_json(\"/home/cyj/公开数据/database\",\"train_author.json\")\n",
    "import re\n",
    "from gensim.models import word2vec\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "import codecs\n",
    "import json\n",
    "from os.path import join\n",
    "import pickle\n",
    "import torch\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "\n",
    "result=[]\n",
    "result1=[]\n",
    "result2=[]\n",
    "count1=[]\n",
    "\n",
    "listn1=[]\n",
    "listn2=[]\n",
    "listn3=[]\n",
    "\n",
    "for n,name in enumerate(name_pubs):\n",
    "    ilabel=0\n",
    "    pubs=[] # all papers\n",
    "    labels=[] # ground truth\n",
    "    \n",
    "    for author in name_pubs[name]:\n",
    "        iauthor_pubs = name_pubs[name][author]\n",
    "        for pub in iauthor_pubs:\n",
    "            pubs.append(pub)\n",
    "            labels.append(ilabel)\n",
    "        ilabel += 1\n",
    "        \n",
    "    print (n,name,len(pubs))\n",
    "    #print(pubs)\n",
    "    #count1.append(len(pubs))\n",
    "    \n",
    "    \n",
    "    if len(pubs)==0:\n",
    "        result.append(0)\n",
    "        continue\n",
    "    \n",
    "    ##保存关系\n",
    "    ###############################################################\n",
    "    name_pubs_raw = {}\n",
    "    for i,pid in enumerate(pubs):\n",
    "        name_pubs_raw[pid] = pubs_raw[pid]\n",
    "        \n",
    "    dump_json(name_pubs_raw, '/home/cyj/hin+A+O/result', name+'.json', indent=4)\n",
    "    save_relation(name+'.json', name)  \n",
    "    ###############################################################\n",
    "    \n",
    "    \n",
    "   \n",
    "    ''' ##元路径游走类\n",
    "    ###############################################################r\n",
    "    mpg = MetaPathGenerator()\n",
    "    mpg.read_data(\"/home/cyj/数据/f1\")\n",
    "    ###############################################################'''\n",
    "    \n",
    "\n",
    "    \n",
    "    ##论文关系表征向量\n",
    "    ############################################################### \n",
    "    # \n",
    "    tt=save_generatenode(name+'.json', name)\n",
    "    window = 5\n",
    "    walk = 5\n",
    "    walk_length = 20\n",
    "    embed_size = 100\n",
    "    neg = 3\n",
    "    sigmoid_reg = True\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f'device = {device}')\n",
    "\n",
    "    # set dataset [PLEASE USE YOUR OWN DATASET TO REPLACE THIS]\n",
    "    demo_edge = tt[0]\n",
    "\n",
    "    edges = [demo_edge]\n",
    "\n",
    "    hin = load_a_HIN_from_pandas(edges)\n",
    "    hin.window = window\n",
    "    dataset = NSTrainSet(hin.sample(walk_length, walk), hin.node_size, neg=neg)\n",
    "    hin2vec = HIN2vec(hin.node_size, hin.path_size, embed_size, sigmoid_reg)\n",
    "\n",
    "    n_epoch = 5\n",
    "    batch_size = 20\n",
    "    log_interval = 200\n",
    "\n",
    "    data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    optimizer = optim.AdamW(hin2vec.parameters())  # 原作者使用的是SGD？ 这里使用AdamW\n",
    "    loss_function = nn.BCELoss()\n",
    "    for epoch in range(n_epoch):\n",
    "        train(log_interval, hin2vec, device, data_loader, optimizer, loss_function, epoch)\n",
    "    \n",
    "    namelist=list(hin.id2node.values())\n",
    "    node_embeds = pd.DataFrame(hin2vec.start_embeds.weight.data.cpu().numpy()).T\n",
    "\n",
    "    #all_embs=[]\n",
    "    embs=[]\n",
    "    cp = set()\n",
    "    for i,pid in enumerate(pubs):\n",
    "        if pid in namelist:\n",
    "            for j in range(len(namelist)):\n",
    "                if pid == namelist[j]:\n",
    "                    embs.append(node_embeds[j].values)\n",
    "        else:\n",
    "            cp.add(i)\n",
    "            embs.append(np.zeros(100))\n",
    "    #all_embs.append(embs)\n",
    "    all_embs= np.array(embs)\n",
    "    print ('relational outlier:',cp)\n",
    "    ############################################################### \n",
    "\n",
    "    \n",
    "    \n",
    "    ##论文文本表征向量\n",
    "    ###############################################################   \n",
    "    ptext_emb=load_data('/home/cyj/hin+A+O/f1','ptext_emb.pkl')\n",
    "    tcp=load_data('/home/cyj/hin+A+O/f1','tcp.pkl')\n",
    "    print ('semantic outlier:',tcp)\n",
    "    tembs=[]\n",
    "    for i,pid in enumerate(pubs):\n",
    "        tembs.append(ptext_emb[pid])\n",
    "    ############################################################### \n",
    "    \n",
    "    ##离散点\n",
    "    outlier=set()\n",
    "    for i in cp:\n",
    "        outlier.add(i)\n",
    "    for i in tcp:\n",
    "        outlier.add(i)\n",
    "    #print('离散点:',outlier)\n",
    "    \n",
    "    ##网络嵌入向量相似度\n",
    "    #sk_sim = np.zeros((len(pubs),len(pubs)))\n",
    "    #for k in range(rw_num):\n",
    "    sk_sim = pairwise_distances(all_embs,metric=\"cosine\")\n",
    "    #sk_sim =sk_sim/rw_num    \n",
    "    \n",
    "    ##文本相似度\n",
    "    t_sim = pairwise_distances(tembs,metric=\"cosine\")\n",
    "    \n",
    "    w=1\n",
    "    sim = (np.array(sk_sim) + w*np.array(t_sim))/(1+w)\n",
    "    \n",
    "    \n",
    "    \n",
    "    ##evaluate\n",
    "    ###############################################################\n",
    "    pre = DBSCAN(eps = 0.2, min_samples = 4,metric =\"precomputed\").fit_predict(sim)\n",
    "    #print(pre)\n",
    "    #print(type(pre))\n",
    "    #for i in range(len(pre)):\n",
    "     #   print('pre',[i],pre[i])\n",
    "    #print('len:',len(pre))\n",
    "    \n",
    "    for i in range(len(pre)):\n",
    "        if pre[i]==-1:\n",
    "            outlier.add(i)\n",
    "    #print('outlier:',outlier)\n",
    "    ## assign each outlier a label\n",
    "    paper_pair = generate_pair(pubs,outlier)\n",
    "    #print('paper_pair:',paper_pair)\n",
    "    paper_pair1 = paper_pair.copy()\n",
    "    K = len(set(pre))\n",
    "    \n",
    "    listap=[]\n",
    "    for i in range(len(pubs)):\n",
    "        listap.append(float(-1))\n",
    "    na1=np.array(listap)\n",
    "    \n",
    "    \n",
    "    for i in range(len(pre)):\n",
    "        if i not in outlier:\n",
    "            continue\n",
    "        j = np.argmax(paper_pair[i])\n",
    "        #print('j1:',j)\n",
    "        while j in outlier:\n",
    "            paper_pair[i][j]=-1\n",
    "            if any(na1==paper_pair[i])  == True:\n",
    "                break\n",
    "            else:\n",
    "                j = np.argmax(paper_pair[i])\n",
    "            #print('j2:',j)\n",
    "        if paper_pair[i][j]>=1.5:\n",
    "            pre[i]=pre[j]\n",
    "        else:\n",
    "            pre[i]=K\n",
    "            K=K+1\n",
    "    \n",
    "    ## find nodes in outlier is the same label or not\n",
    "    for ii,i in enumerate(outlier):\n",
    "        for jj,j in enumerate(outlier):\n",
    "            if jj<=ii:\n",
    "                continue\n",
    "            else:\n",
    "                if paper_pair1[i][j]>=1.5:\n",
    "                    pre[j]=pre[i]\n",
    "            \n",
    "            \n",
    "    \n",
    "    labels = np.array(labels)\n",
    "    pre = np.array(pre)\n",
    "    print (labels,len(set(labels)))\n",
    "    print (pre,len(set(pre)))\n",
    "    pairwise_precision, pairwise_recall, pairwise_f1, a, b, c = pairwise_evaluate(labels,pre)\n",
    "    print (pairwise_precision, pairwise_recall, pairwise_f1, a, b, c)\n",
    "    result.append(pairwise_precision)\n",
    "    result1.append(pairwise_recall)\n",
    "    result2.append(pairwise_f1)\n",
    "\n",
    "    print ('avg_pre:', np.mean(result),'avg_recall:', np.mean(result1),'avg_f1:', np.mean(result2))\n",
    "    listn1.append(np.mean(result))\n",
    "    listn2.append(np.mean(result1))\n",
    "    listn3.append(np.mean(result2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
